{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/monko9j1/making_lofi/blob/main/making_lofi.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CKFcWCo9Kbee"
      },
      "source": [
        "# INSTALLS AND LIBRARIES\n",
        "\n",
        "First, we have to make sure our file has our Database (cloned from github), and a few other libraries that it might not automatically install.\n",
        "\n",
        "Then, we'll use a few different libraries, including pandas (popular machine learning library), Music21 (translates midi files into numbers which a computer can process), and Keras (enables the creation of more advanced neural networks)."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7lSYnG_hal4O",
        "outputId": "d94d5b30-d79f-4649-b4c0-966401a34f58",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 129
        }
      },
      "source": [
        "! git clone https://github.com/nmtremblay/lofi-samples.git\n",
        "! pip install music21\n",
        "! pip install np_utils\n",
        "! pip install pygame"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "fatal: destination path 'lofi-samples' already exists and is not an empty directory.\n",
            "Requirement already satisfied: music21 in /usr/local/lib/python3.6/dist-packages (5.5.0)\n",
            "Requirement already satisfied: np_utils in /usr/local/lib/python3.6/dist-packages (0.5.12.1)\n",
            "Requirement already satisfied: numpy>=1.0 in /usr/local/lib/python3.6/dist-packages (from np_utils) (1.17.5)\n",
            "Requirement already satisfied: future>=0.16 in /usr/local/lib/python3.6/dist-packages (from np_utils) (0.16.0)\n",
            "Requirement already satisfied: pygame in /usr/local/lib/python3.6/dist-packages (1.9.6)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JRH2IZCK5Zh3",
        "outputId": "c0bc1445-8b7d-42f1-ca91-05e9dc2e1df5",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 84
        }
      },
      "source": [
        "import glob\n",
        "import pickle\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from music21 import *\n",
        "from keras.models import Sequential\n",
        "from keras.layers import Dense\n",
        "from keras.layers import Dropout\n",
        "from keras.layers import LSTM, Bidirectional\n",
        "from keras.layers import Activation\n",
        "from keras.layers import BatchNormalization as BatchNorm\n",
        "from keras.utils import np_utils\n",
        "from keras.callbacks import ModelCheckpoint\n",
        "import tensorflow as tf\n",
        "from pygame import *\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Using TensorFlow backend.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "<p style=\"color: red;\">\n",
              "The default version of TensorFlow in Colab will soon switch to TensorFlow 2.x.<br>\n",
              "We recommend you <a href=\"https://www.tensorflow.org/guide/migrate\" target=\"_blank\">upgrade</a> now \n",
              "or ensure your notebook will continue to use TensorFlow 1.x via the <code>%tensorflow_version 1.x</code> magic:\n",
              "<a href=\"https://colab.research.google.com/notebooks/tensorflow_version.ipynb\" target=\"_blank\">more info</a>.</p>\n"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "I0fbwcf2LEG9"
      },
      "source": [
        "Now, we'll convert our MIDI data into data which Music21 can process."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MT8rw-xshqBN"
      },
      "source": [
        "#creating an empty list to hold the notes in\n",
        "notes = []\n",
        "\n",
        "#this for loop goes through each midi file and flattens out the notes inside of it\n",
        "for file in glob.glob(\"lofi-samples/samples/*.mid\"):\n",
        "    midi = converter.parse(file)\n",
        "    notes_to_parse = midi.flat.notes\n",
        "\n",
        "    for element in notes_to_parse:\n",
        "        if isinstance(element, note.Note): #if it's a single note, we don't have to join it to any other notes in the series\n",
        "            notes.append(str(element.pitch))\n",
        "        elif isinstance(element, chord.Chord): #if it's a chord, we will have to join it to the other notes\n",
        "            notes.append('.'.join(str(n) for n in element.normalOrder))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yPP2hCLfwooO"
      },
      "source": [
        "# DATA FORMATTING\n",
        "\n",
        "Here, we're going to convert all the midis from the dataset into sequential lists of notes and chords. This is important, of course, because our network operates on sequential data."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "m4M8DBK2xYWV"
      },
      "source": [
        "First, we have to translate all this categorical (string-based) data into numerical (integer-based) data. This can be accomplished with a mapping function."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TkznkqnWjM7l"
      },
      "source": [
        "#this is the amount of previous notes our algorithm will use to predict the next notes\n",
        "#mess around with this number to see how this impacts the accuracy\n",
        "sequence_length = 20 #our chord progressions are pretty short so we might not need that many notes\n",
        "\n",
        "# get all pitch names\n",
        "pitchnames = sorted(set(item for item in notes))\n",
        "\n",
        "# create a dictionary to map pitches to integers\n",
        "note_to_int = dict((note, number) for number, note in enumerate(pitchnames))\n",
        "\n",
        "network_input = []\n",
        "network_output = []\n",
        "\n",
        "# create input sequences and the corresponding outputs\n",
        "for i in range(0, len(notes) - sequence_length, 1):\n",
        "    sequence_in = notes[i:i + sequence_length]\n",
        "    sequence_out = notes[i + sequence_length]\n",
        "    network_input.append([note_to_int[char] for char in sequence_in])\n",
        "    network_output.append(note_to_int[sequence_out])\n",
        "    n_patterns = len(network_input)\n",
        "\n",
        "# reshape the input vector into a format compatible with LSTM layers\n",
        "network_input = np.reshape(network_input, (n_patterns, sequence_length, 1))\n",
        "# normalizing the input values\n",
        "n_vocab = len(set(notes))\n",
        "network_input = network_input / float(n_vocab)\n",
        "\n",
        "#i wrote this to bypass an error message if there's no output yet (bc we haven't trained the model)\n",
        "from keras.utils.np_utils import to_categorical\n",
        "try:\n",
        "  network_output = np_utils.to_categorical(network_output)\n",
        "except ValueError:  #raised if `y` is empty.\n",
        "    pass"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5_MjDeJG-a_J"
      },
      "source": [
        "#THE NETWORK!\n",
        "\n",
        "We're working with a structure with three LSTM layers, three Dropout layers, two Dense layers, and one Activation layer.\n",
        "\n",
        "Play around with the architecture to see if you can improve the quality of the predictions!"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OUT1dbzVB4pi",
        "outputId": "f4e9c531-8dc5-43c2-b8b4-e3309c99cccc",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 317
        }
      },
      "source": [
        "model = Sequential()\n",
        "\n",
        "#each model.add command adds a new layer to our sequential model\n",
        "#this one is our input layer :)\n",
        "model.add(LSTM(\n",
        "        256, #nodes\n",
        "        input_shape=(network_input.shape[1], network_input.shape[2]), #unique input to tell the network the shape of our data\n",
        "        return_sequences=True #this means we'll return sequences of data\n",
        "    ))\n",
        "\n",
        "#each one of these is a hidden layer:\n",
        "#in theory they would all be togeter but I'm commeting fuck u\n",
        "\n",
        "model.add(Dropout(0.3))\n",
        "#these layers will set a fraction of inputs (in in this case 3/10) to 0 at each update.\n",
        "#it's a technique to prevent overfitting\n",
        "#(in case you haven't heard, the fraction of input units we're dropping during training is our first parameter)\n",
        "\n",
        "model.add(LSTM(512, return_sequences=True))\n",
        "#each type of LSTM layer takes a sequence as an input and returns either sequences or matrixes\n",
        "#here, the first parameter is how many nodes our layer will have.\n",
        "#(same thing with all the non-dropout layers)\n",
        "\n",
        "model.add(Dropout(0.3))\n",
        "model.add(LSTM(256))\n",
        "model.add(Dense(256))\n",
        "#these guys are fully connected and attatch to an output node\n",
        "\n",
        "model.add(Dropout(0.3))\n",
        "model.add(Dense(n_vocab))\n",
        "#because this one's our last layer, it should have the same amount of nodes as the number of different outputs our system has\n",
        "#this will make sure the network's output will map right onto the system classes\n",
        "\n",
        "model.add(Activation('softmax'))\n",
        "#this one figures out which activation function to use to calculate the output\n",
        "\n",
        "model.compile(loss='categorical_crossentropy', optimizer='rmsprop')\n",
        "#this is our training command\n",
        "#we're using categorical"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:66: The name tf.get_default_graph is deprecated. Please use tf.compat.v1.get_default_graph instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:541: The name tf.placeholder is deprecated. Please use tf.compat.v1.placeholder instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:4432: The name tf.random_uniform is deprecated. Please use tf.random.uniform instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:148: The name tf.placeholder_with_default is deprecated. Please use tf.compat.v1.placeholder_with_default instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:3733: calling dropout (from tensorflow.python.ops.nn_ops) with keep_prob is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Please use `rate` instead of `keep_prob`. Rate should be set to `rate = 1 - keep_prob`.\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/optimizers.py:793: The name tf.train.Optimizer is deprecated. Please use tf.compat.v1.train.Optimizer instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:3576: The name tf.log is deprecated. Please use tf.math.log instead.\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kIIOhiqsUBOK",
        "outputId": "88d4ba59-159f-4587-ecbf-4d86aa92dbd4",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "model = Sequential()\n",
        "\n",
        "model.add(LSTM(256, input_shape=(network_input.shape[1], network_input.shape[2]), return_sequences=True))\n",
        "model.add(Dropout(0.2))\n",
        "model.add(LSTM(256, return_sequences=True))\n",
        "model.add(Dropout(0.2))\n",
        "model.add(LSTM(256, input_shape=(network_input.shape[1], network_input.shape[2]), return_sequences=False))\n",
        "model.add(Dropout(0.2))\n",
        "model.add(Dense(n_vocab))\n",
        "model.add(Activation('softmax'))\n",
        "model.compile(loss='categorical_crossentropy', optimizer='rmsprop')\n",
        "\n",
        "model.fit(network_input, network_output, epochs=20, batch_size=64)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow_core/python/ops/math_grad.py:1424: where (from tensorflow.python.ops.array_ops) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Use tf.where in 2.0, which has the same broadcast rule as np.where\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:1033: The name tf.assign_add is deprecated. Please use tf.compat.v1.assign_add instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:1020: The name tf.assign is deprecated. Please use tf.compat.v1.assign instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:3005: The name tf.Session is deprecated. Please use tf.compat.v1.Session instead.\n",
            "\n",
            "Epoch 1/20\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:190: The name tf.get_default_session is deprecated. Please use tf.compat.v1.get_default_session instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:197: The name tf.ConfigProto is deprecated. Please use tf.compat.v1.ConfigProto instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:207: The name tf.global_variables is deprecated. Please use tf.compat.v1.global_variables instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:216: The name tf.is_variable_initialized is deprecated. Please use tf.compat.v1.is_variable_initialized instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:223: The name tf.variables_initializer is deprecated. Please use tf.compat.v1.variables_initializer instead.\n",
            "\n",
            "1544/1544 [==============================] - 15s 9ms/step - loss: 5.2849\n",
            "Epoch 2/20\n",
            "1544/1544 [==============================] - 13s 8ms/step - loss: 5.1565\n",
            "Epoch 3/20\n",
            "1544/1544 [==============================] - 13s 8ms/step - loss: 5.1406\n",
            "Epoch 4/20\n",
            "1544/1544 [==============================] - 12s 8ms/step - loss: 5.1292\n",
            "Epoch 5/20\n",
            "1544/1544 [==============================] - 13s 8ms/step - loss: 5.1239\n",
            "Epoch 6/20\n",
            "1544/1544 [==============================] - 13s 8ms/step - loss: 5.1225\n",
            "Epoch 7/20\n",
            "1544/1544 [==============================] - 12s 8ms/step - loss: 5.1201\n",
            "Epoch 8/20\n",
            "1544/1544 [==============================] - 13s 8ms/step - loss: 5.0859\n",
            "Epoch 9/20\n",
            "1544/1544 [==============================] - 13s 8ms/step - loss: 5.0493\n",
            "Epoch 10/20\n",
            "1544/1544 [==============================] - 13s 8ms/step - loss: 5.0098\n",
            "Epoch 11/20\n",
            "1544/1544 [==============================] - 13s 8ms/step - loss: 4.9842\n",
            "Epoch 12/20\n",
            "1544/1544 [==============================] - 13s 8ms/step - loss: 4.9542\n",
            "Epoch 13/20\n",
            "1544/1544 [==============================] - 13s 8ms/step - loss: 4.9003\n",
            "Epoch 14/20\n",
            "1544/1544 [==============================] - 13s 8ms/step - loss: 4.8752\n",
            "Epoch 15/20\n",
            "1544/1544 [==============================] - 13s 8ms/step - loss: 4.8440\n",
            "Epoch 16/20\n",
            "1544/1544 [==============================] - 13s 8ms/step - loss: 4.8003\n",
            "Epoch 17/20\n",
            "1544/1544 [==============================] - 13s 8ms/step - loss: 4.7762\n",
            "Epoch 18/20\n",
            "1544/1544 [==============================] - 13s 8ms/step - loss: 4.7101\n",
            "Epoch 19/20\n",
            "1544/1544 [==============================] - 13s 8ms/step - loss: 4.6963\n",
            "Epoch 20/20\n",
            "1544/1544 [==============================] - 13s 8ms/step - loss: 4.6612\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<keras.callbacks.History at 0x7f67de92ecf8>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 7
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dnWJVvmtGhKb"
      },
      "source": [
        "# Making Music!\n",
        "Here, we're setting up the network exactly the same as the last one. This time, instead of training the set, we'll use the weights generated by the network during the training phase.\n",
        "\n",
        "The first segment of code generates the music21 note values of our song."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "K7wIGfa0U9Ak"
      },
      "source": [
        "model = Sequential()\n",
        "model.add(LSTM(\n",
        "    512,\n",
        "    input_shape=(network_input.shape[1], network_input.shape[2]),\n",
        "    return_sequences=True\n",
        "))\n",
        "model.add(Dropout(0.3))\n",
        "model.add(LSTM(512, return_sequences=True))\n",
        "model.add(Dropout(0.3))\n",
        "model.add(LSTM(512))\n",
        "model.add(Dense(256))\n",
        "model.add(Dropout(0.3))\n",
        "model.add(Dense(n_vocab))\n",
        "model.add(Activation('softmax'))\n",
        "model.compile(loss='categorical_crossentropy', optimizer='rmsprop')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8gUFYfDuHS45"
      },
      "source": [
        "Just like we used a mapping function to format the data to fit into our neural network, we'll have to use another mapping function to \"un-format\" it back into a form in which it can be played back. Eagle-eyed readers will notice how similar it looks to the mapping function we used before."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "e_0JruFsVFsb"
      },
      "source": [
        "start = np.random.randint(0, len(network_input)-1)\n",
        "int_to_note = dict((number, note) for number, note in enumerate(pitchnames))\n",
        "\n",
        "pattern = network_input[start]\n",
        "\n",
        "prediction_output = []\n",
        "\n",
        "for note_index in range(100): #here, we're generating 100 notes\n",
        "    prediction_input = np.reshape(pattern, (1, len(pattern), 1))\n",
        "    prediction_input = prediction_input / float(n_vocab)\n",
        "\n",
        "    prediction = model.predict(prediction_input, verbose=0)\n",
        "    index = np.argmax(prediction)\n",
        "\n",
        "    result = int_to_note[index]\n",
        "    prediction_output.append(result)\n",
        "\n",
        "    pattern.ravel()\n",
        "    patternbeta =  pattern + index\n",
        "    patternbeta = patternbeta[1:len(patternbeta)]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "m2WZMsb_IfX9"
      },
      "source": [
        "Finally, we can organize our notes into phrases. Specifically, Note objects and Chord objects.\n",
        "\n",
        "If our Music21 value is a single note, we can store it in the corresponding Note object and play it with a piano sound."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "G7KlpaMoINgw"
      },
      "source": [
        "offset = 0\n",
        "output_notes = []\n",
        "\n",
        "for pattern in prediction_output:\n",
        "    #chords!\n",
        "    if ('.' in pattern) or pattern.isdigit():\n",
        "        notes_in_chord = pattern.split('.')\n",
        "        notes = [] #creating the array where we'll store the note values, which the for loop below will handle\n",
        "        for current_note in notes_in_chord:\n",
        "            new_note = note.Note(int(current_note))\n",
        "            new_note.storedInstrument = instrument.Piano()\n",
        "            notes.append(new_note)\n",
        "        new_chord = chord.Chord(notes) #adding the note to the chord object\n",
        "        new_chord.offset = offset #connecting it to the offset variable\n",
        "        output_notes.append(new_chord) #adding it to the song\n",
        "    #notes!\n",
        "    else:\n",
        "        new_note = note.Note(pattern) #storing it in the object\n",
        "        new_note.offset = offset #connecting it to our offset command later on\n",
        "        new_note.storedInstrument = instrument.Piano() #playing it with piano\n",
        "        output_notes.append(new_note) #adding it to the song\n",
        "    #make sure notes don't end up on top of each other by adding an 0.5 offset every time\n",
        "    offset += 0.5"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6TGw_ewHJlRi"
      },
      "source": [
        "Our final command aggregates all of our notes into a single Stream object, then uses the write function to convert it into a playable MIDI file.\n",
        "\n",
        "Finally! We can play our song :)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wFsyVI5NaPhA",
        "outputId": "80826cb4-ae25-42c2-894b-f9124572bd02",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 36
        }
      },
      "source": [
        "us = environment.UserSettings()\n",
        "us.getSettingsPath()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "PosixPath('/root/.music21rc')"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 14
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "88fdum7TIMIh",
        "outputId": "861123ef-b493-4ed6-c2ef-95f9a0a5d842",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 17
        }
      },
      "source": [
        "s = stream.Stream(output_notes)\n",
        "mf = s.write('midi', fp=\"lofi-samples/testOutput.mid\")\n",
        "\n",
        "#from here, i opened the instrumental and the drum loop file in audacity and played them together!"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "\n",
              "                <div id='midiPlayerDiv17176'></div>\n",
              "                <link rel=\"stylesheet\" href=\"//cuthbertLab.github.io/music21j/css/m21.css\"\n",
              "                    type=\"text/css\" />\n",
              "                <script>\n",
              "                require.config({\n",
              "                    paths: {'music21': '//cuthbertLab.github.io/music21j/src/music21'}\n",
              "                });\n",
              "                require(['music21'], function() {\n",
              "                               mp = new music21.miditools.MidiPlayer();\n",
              "                               mp.addPlayer('#midiPlayerDiv17176');\n",
              "                               mp.base64Load('data:audio/midi;base64,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');\n",
              "                        });\n",
              "                </script>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    }
  ]
}